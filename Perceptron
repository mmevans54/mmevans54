{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2380,"sourceType":"datasetVersion","datasetId":1320}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook contains two parts. **Part 1, Testing Perceptrons**, provides you an opportunity to demonstrate your ability to apply course concepts by implementing a test function for a perceptron. **Part 2, Mine Detection**, provides you an opportunity to practice using widely-used ML libraries and an ML workflow to solve a classification problem.\n\nYou do not need to complete Part 1 in order to complete Part 2. If you get stuck on Part 1, and choose to work on Part 2, be sure that all of your code for Part 1 runs without error. You can comment out your code in Part 1 if necessary.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Testing Perceptrons\n\nGiven a simple Perceptron classifier, and a test set cancer diagnoses, demonstrate your ability to implement a perceptron's `test` function, such that it returns the accuracy of its predictions for the class labels of samples in a training set.\n\n## The Perceptron Implementation\n\nLet's first introduce the classifier, which you should find familiar, and you do not need to modify. Notice that the `test` method is stubbed.","metadata":{}},{"cell_type":"code","source":"import random\n\nclass Perceptron:\n\n    def __init__(self, alpha = 0.1, max_epochs = 1000):\n        self.alpha = alpha\n        self.max_epochs = max_epochs\n\n    def train(self, training_set):\n        self.weights = self._initial_weights(len(training_set[0]))\n        for i in range(self.max_epochs):\n            correct_classifications = 0\n            for record in training_set:\n                y = record[0]\n                y_hat = self.predict(record[1:])\n                if y == y_hat: correct_classifications += 1\n                self._update_weights(y - y_hat, [1] + record[1:])\n            if correct_classifications / len(training_set) == 1.0:\n                print(f\"Epoch {i} Accuracy: {correct_classifications / len(training_set)}\")\n                break\n\n    def predict(self, features):\n        return self._sign_of(self._dot_product(self.weights, [1] + features))\n\n    def test(self, test_set):\n        pass\n\n    def _update_weights(self, error, features):\n        self.weights[0] += self.alpha * error\n        for i in range(1, len(self.weights)):\n            self.weights[i] += self.alpha * error * features[i]\n\n    def _dot_product(self, w, x):\n        return sum([w * x for w, x in zip(w, x)])\n\n    def _sign_of(self, value):\n        return 1 if value >= 0 else -1\n\n    def _initial_weights(self, length):\n        return [random.uniform(0, 1) for _ in range(length)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-28T04:02:32.814398Z","iopub.execute_input":"2024-05-28T04:02:32.814818Z","iopub.status.idle":"2024-05-28T04:02:32.857795Z","shell.execute_reply.started":"2024-05-28T04:02:32.814779Z","shell.execute_reply":"2024-05-28T04:02:32.856466Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## The Data Set\n\nThere is no need for you to manually load the data set. We have provided a subset of the cancer diagnoses data set here, already split into a training set, `diagnoses_training`, and a test set, `diagnoses_test`. Each data set is a simple, two-dimensional Python list, where each sub-list represents the attributes for one diagnosis.\n\nWe have preprocessed the data, such that the **first dimension is the class label**: a `1` indicating malignant, and `-1` indicating benign.\n","metadata":{}},{"cell_type":"code","source":"diagnoses_training = [\n    [1, 17.99, 10.38, 122.8, 1001, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193, 25.38, 17.33, 184.6, 2019, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189],\n    [1, 20.57, 17.77, 132.9, 1326, 0.08474, 0.07864, 0.0869, 0.07017, 0.1812, 0.05667, 0.5435, 0.7339, 3.398, 74.08, 0.005225, 0.01308, 0.0186, 0.0134, 0.01389, 0.003532, 24.99, 23.41, 158.8, 1956, 0.1238, 0.1866, 0.2416, 0.186, 0.275, 0.08902],\n    [1, 19.69, 21.25, 130, 1203, 0.1096, 0.1599, 0.1974, 0.1279, 0.2069, 0.05999, 0.7456, 0.7869, 4.585, 94.03, 0.00615, 0.04006, 0.03832, 0.02058, 0.0225, 0.004571, 23.57, 25.53, 152.5, 1709, 0.1444, 0.4245, 0.4504, 0.243, 0.3613, 0.08758],\n    [1, 11.42, 20.38, 77.58, 386.1, 0.1425, 0.2839, 0.2414, 0.1052, 0.2597, 0.09744, 0.4956, 1.156, 3.445, 27.23, 0.00911, 0.07458, 0.05661, 0.01867, 0.05963, 0.009208, 14.91, 26.5, 98.87, 567.7, 0.2098, 0.8663, 0.6869, 0.2575, 0.6638, 0.173],\n    [1, 20.29, 14.34, 135.1, 1297, 0.1003, 0.1328, 0.198, 0.1043, 0.1809, 0.05883, 0.7572, 0.7813, 5.438, 94.44, 0.01149, 0.02461, 0.05688, 0.01885, 0.01756, 0.005115, 22.54, 16.67, 152.2, 1575, 0.1374, 0.205, 0.4, 0.1625, 0.2364, 0.07678],\n    [1, 12.45, 15.7, 82.57, 477.1, 0.1278, 0.17, 0.1578, 0.08089, 0.2087, 0.07613, 0.3345, 0.8902, 2.217, 27.19, 0.00751, 0.03345, 0.03672, 0.01137, 0.02165, 0.005082, 15.47, 23.75, 103.4, 741.6, 0.1791, 0.5249, 0.5355, 0.1741, 0.3985, 0.1244],\n    [1, 18.25, 19.98, 119.6, 1040, 0.09463, 0.109, 0.1127, 0.074, 0.1794, 0.05742, 0.4467, 0.7732, 3.18, 53.91, 0.004314, 0.01382, 0.02254, 0.01039, 0.01369, 0.002179, 22.88, 27.66, 153.2, 1606, 0.1442, 0.2576, 0.3784, 0.1932, 0.3063, 0.08368],\n    [1, 13.71, 20.83, 90.2, 577.9, 0.1189, 0.1645, 0.09366, 0.05985, 0.2196, 0.07451, 0.5835, 1.377, 3.856, 50.96, 0.008805, 0.03029, 0.02488, 0.01448, 0.01486, 0.005412, 17.06, 28.14, 110.6, 897, 0.1654, 0.3682, 0.2678, 0.1556, 0.3196, 0.1151],\n    [1, 13, 21.82, 87.5, 519.8, 0.1273, 0.1932, 0.1859, 0.09353, 0.235, 0.07389, 0.3063, 1.002, 2.406, 24.32, 0.005731, 0.03502, 0.03553, 0.01226, 0.02143, 0.003749, 15.49, 30.73, 106.2, 739.3, 0.1703, 0.5401, 0.539, 0.206, 0.4378, 0.1072],\n    [1, 12.46, 24.04, 83.97, 475.9, 0.1186, 0.2396, 0.2273, 0.08543, 0.203, 0.08243, 0.2976, 1.599, 2.039, 23.94, 0.007149, 0.07217, 0.07743, 0.01432, 0.01789, 0.01008, 15.09, 40.68, 97.65, 711.4, 0.1853, 1.058, 1.105, 0.221, 0.4366, 0.2075],\n    [1, 16.02, 23.24, 102.7, 797.8, 0.08206, 0.06669, 0.03299, 0.03323, 0.1528, 0.05697, 0.3795, 1.187, 2.466, 40.51, 0.004029, 0.009269, 0.01101, 0.007591, 0.0146, 0.003042, 19.19, 33.88, 123.8, 1150, 0.1181, 0.1551, 0.1459, 0.09975, 0.2948, 0.08452],\n    [1, 15.78, 17.89, 103.6, 781, 0.0971, 0.1292, 0.09954, 0.06606, 0.1842, 0.06082, 0.5058, 0.9849, 3.564, 54.16, 0.005771, 0.04061, 0.02791, 0.01282, 0.02008, 0.004144, 20.42, 27.28, 136.5, 1299, 0.1396, 0.5609, 0.3965, 0.181, 0.3792, 0.1048],\n    [1, 19.17, 24.8, 132.4, 1123, 0.0974, 0.2458, 0.2065, 0.1118, 0.2397, 0.078, 0.9555, 3.568, 11.07, 116.2, 0.003139, 0.08297, 0.0889, 0.0409, 0.04484, 0.01284, 20.96, 29.94, 151.7, 1332, 0.1037, 0.3903, 0.3639, 0.1767, 0.3176, 0.1023],\n    [1, 15.85, 23.95, 103.7, 782.7, 0.08401, 0.1002, 0.09938, 0.05364, 0.1847, 0.05338, 0.4033, 1.078, 2.903, 36.58, 0.009769, 0.03126, 0.05051, 0.01992, 0.02981, 0.003002, 16.84, 27.66, 112, 876.5, 0.1131, 0.1924, 0.2322, 0.1119, 0.2809, 0.06287],\n    [1, 13.73, 22.61, 93.6, 578.3, 0.1131, 0.2293, 0.2128, 0.08025, 0.2069, 0.07682, 0.2121, 1.169, 2.061, 19.21, 0.006429, 0.05936, 0.05501, 0.01628, 0.01961, 0.008093, 15.03, 32.01, 108.8, 697.7, 0.1651, 0.7725, 0.6943, 0.2208, 0.3596, 0.1431],\n    [1, 14.54, 27.54, 96.73, 658.8, 0.1139, 0.1595, 0.1639, 0.07364, 0.2303, 0.07077, 0.37, 1.033, 2.879, 32.55, 0.005607, 0.0424, 0.04741, 0.0109, 0.01857, 0.005466, 17.46, 37.13, 124.1, 943.2, 0.1678, 0.6577, 0.7026, 0.1712, 0.4218, 0.1341],\n    [1, 14.68, 20.13, 94.74, 684.5, 0.09867, 0.072, 0.07395, 0.05259, 0.1586, 0.05922, 0.4727, 1.24, 3.195, 45.4, 0.005718, 0.01162, 0.01998, 0.01109, 0.0141, 0.002085, 19.07, 30.88, 123.4, 1138, 0.1464, 0.1871, 0.2914, 0.1609, 0.3029, 0.08216],\n    [1, 16.13, 20.68, 108.1, 798.8, 0.117, 0.2022, 0.1722, 0.1028, 0.2164, 0.07356, 0.5692, 1.073, 3.854, 54.18, 0.007026, 0.02501, 0.03188, 0.01297, 0.01689, 0.004142, 20.96, 31.48, 136.8, 1315, 0.1789, 0.4233, 0.4784, 0.2073, 0.3706, 0.1142],\n    [1, 19.81, 22.15, 130, 1260, 0.09831, 0.1027, 0.1479, 0.09498, 0.1582, 0.05395, 0.7582, 1.017, 5.865, 112.4, 0.006494, 0.01893, 0.03391, 0.01521, 0.01356, 0.001997, 27.32, 30.88, 186.8, 2398, 0.1512, 0.315, 0.5372, 0.2388, 0.2768, 0.07615],\n    [-1, 13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766, 0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023, 15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259],\n    [-1, 13.08, 15.71, 85.63, 520, 0.1075, 0.127, 0.04568, 0.0311, 0.1967, 0.06811, 0.1852, 0.7477, 1.383, 14.67, 0.004097, 0.01898, 0.01698, 0.00649, 0.01678, 0.002425, 14.5, 20.49, 96.09, 630.5, 0.1312, 0.2776, 0.189, 0.07283, 0.3184, 0.08183],\n    [-1, 9.504, 12.44, 60.34, 273.9, 0.1024, 0.06492, 0.02956, 0.02076, 0.1815, 0.06905, 0.2773, 0.9768, 1.909, 15.7, 0.009606, 0.01432, 0.01985, 0.01421, 0.02027, 0.002968, 10.23, 15.66, 65.13, 314.9, 0.1324, 0.1148, 0.08867, 0.06227, 0.245, 0.07773],\n    [1, 15.34, 14.26, 102.5, 704.4, 0.1073, 0.2135, 0.2077, 0.09756, 0.2521, 0.07032, 0.4388, 0.7096, 3.384, 44.91, 0.006789, 0.05328, 0.06446, 0.02252, 0.03672, 0.004394, 18.07, 19.08, 125.1, 980.9, 0.139, 0.5954, 0.6305, 0.2393, 0.4667, 0.09946],\n    [1, 21.16, 23.04, 137.2, 1404, 0.09428, 0.1022, 0.1097, 0.08632, 0.1769, 0.05278, 0.6917, 1.127, 4.303, 93.99, 0.004728, 0.01259, 0.01715, 0.01038, 0.01083, 0.001987, 29.17, 35.59, 188, 2615, 0.1401, 0.26, 0.3155, 0.2009, 0.2822, 0.07526],\n    [1, 16.65, 21.38, 110, 904.6, 0.1121, 0.1457, 0.1525, 0.0917, 0.1995, 0.0633, 0.8068, 0.9017, 5.455, 102.6, 0.006048, 0.01882, 0.02741, 0.0113, 0.01468, 0.002801, 26.46, 31.56, 177, 2215, 0.1805, 0.3578, 0.4695, 0.2095, 0.3613, 0.09564],\n    [1, 17.14, 16.4, 116, 912.7, 0.1186, 0.2276, 0.2229, 0.1401, 0.304, 0.07413, 1.046, 0.976, 7.276, 111.4, 0.008029, 0.03799, 0.03732, 0.02397, 0.02308, 0.007444, 22.25, 21.4, 152.4, 1461, 0.1545, 0.3949, 0.3853, 0.255, 0.4066, 0.1059],\n    [1, 14.58, 21.53, 97.41, 644.8, 0.1054, 0.1868, 0.1425, 0.08783, 0.2252, 0.06924, 0.2545, 0.9832, 2.11, 21.05, 0.004452, 0.03055, 0.02681, 0.01352, 0.01454, 0.003711, 17.62, 33.21, 122.4, 896.9, 0.1525, 0.6643, 0.5539, 0.2701, 0.4264, 0.1275],\n    [1, 18.61, 20.25, 122.1, 1094, 0.0944, 0.1066, 0.149, 0.07731, 0.1697, 0.05699, 0.8529, 1.849, 5.632, 93.54, 0.01075, 0.02722, 0.05081, 0.01911, 0.02293, 0.004217, 21.31, 27.26, 139.9, 1403, 0.1338, 0.2117, 0.3446, 0.149, 0.2341, 0.07421],\n    [1, 15.3, 25.27, 102.4, 732.4, 0.1082, 0.1697, 0.1683, 0.08751, 0.1926, 0.0654, 0.439, 1.012, 3.498, 43.5, 0.005233, 0.03057, 0.03576, 0.01083, 0.01768, 0.002967, 20.27, 36.71, 149.3, 1269, 0.1641, 0.611, 0.6335, 0.2024, 0.4027, 0.09876],\n    [1, 17.57, 15.05, 115, 955.1, 0.09847, 0.1157, 0.09875, 0.07953, 0.1739, 0.06149, 0.6003, 0.8225, 4.655, 61.1, 0.005627, 0.03033, 0.03407, 0.01354, 0.01925, 0.003742, 20.01, 19.52, 134.9, 1227, 0.1255, 0.2812, 0.2489, 0.1456, 0.2756, 0.07919],\n    [1, 18.63, 25.11, 124.8, 1088, 0.1064, 0.1887, 0.2319, 0.1244, 0.2183, 0.06197, 0.8307, 1.466, 5.574, 105, 0.006248, 0.03374, 0.05196, 0.01158, 0.02007, 0.00456, 23.15, 34.01, 160.5, 1670, 0.1491, 0.4257, 0.6133, 0.1848, 0.3444, 0.09782],\n    [1, 11.84, 18.7, 77.93, 440.6, 0.1109, 0.1516, 0.1218, 0.05182, 0.2301, 0.07799, 0.4825, 1.03, 3.475, 41, 0.005551, 0.03414, 0.04205, 0.01044, 0.02273, 0.005667, 16.82, 28.12, 119.4, 888.7, 0.1637, 0.5775, 0.6956, 0.1546, 0.4761, 0.1402],\n    [1, 17.02, 23.98, 112.8, 899.3, 0.1197, 0.1496, 0.2417, 0.1203, 0.2248, 0.06382, 0.6009, 1.398, 3.999, 67.78, 0.008268, 0.03082, 0.05042, 0.01112, 0.02102, 0.003854, 20.88, 32.09, 136.1, 1344, 0.1634, 0.3559, 0.5588, 0.1847, 0.353, 0.08482],\n    [1, 19.27, 26.47, 127.9, 1162, 0.09401, 0.1719, 0.1657, 0.07593, 0.1853, 0.06261, 0.5558, 0.6062, 3.528, 68.17, 0.005015, 0.03318, 0.03497, 0.009643, 0.01543, 0.003896, 24.15, 30.9, 161.4, 1813, 0.1509, 0.659, 0.6091, 0.1785, 0.3672, 0.1123],\n    [1, 16.13, 17.88, 107, 807.2, 0.104, 0.1559, 0.1354, 0.07752, 0.1998, 0.06515, 0.334, 0.6857, 2.183, 35.03, 0.004185, 0.02868, 0.02664, 0.009067, 0.01703, 0.003817, 20.21, 27.26, 132.7, 1261, 0.1446, 0.5804, 0.5274, 0.1864, 0.427, 0.1233],\n    [1, 16.74, 21.59, 110.1, 869.5, 0.0961, 0.1336, 0.1348, 0.06018, 0.1896, 0.05656, 0.4615, 0.9197, 3.008, 45.19, 0.005776, 0.02499, 0.03695, 0.01195, 0.02789, 0.002665, 20.01, 29.02, 133.5, 1229, 0.1563, 0.3835, 0.5409, 0.1813, 0.4863, 0.08633],\n    [1, 14.25, 21.72, 93.63, 633, 0.09823, 0.1098, 0.1319, 0.05598, 0.1885, 0.06125, 0.286, 1.019, 2.657, 24.91, 0.005878, 0.02995, 0.04815, 0.01161, 0.02028, 0.004022, 15.89, 30.36, 116.2, 799.6, 0.1446, 0.4238, 0.5186, 0.1447, 0.3591, 0.1014],\n    [-1, 13.03, 18.42, 82.61, 523.8, 0.08983, 0.03766, 0.02562, 0.02923, 0.1467, 0.05863, 0.1839, 2.342, 1.17, 14.16, 0.004352, 0.004899, 0.01343, 0.01164, 0.02671, 0.001777, 13.3, 22.81, 84.46, 545.9, 0.09701, 0.04619, 0.04833, 0.05013, 0.1987, 0.06169],\n    [1, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504, 1.214, 2.188, 8.077, 106, 0.006883, 0.01094, 0.01818, 0.01917, 0.007882, 0.001754, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504],\n    [1, 13.48, 20.82, 88.4, 559.2, 0.1016, 0.1255, 0.1063, 0.05439, 0.172, 0.06419, 0.213, 0.5914, 1.545, 18.52, 0.005367, 0.02239, 0.03049, 0.01262, 0.01377, 0.003187, 15.53, 26.02, 107.3, 740.4, 0.161, 0.4225, 0.503, 0.2258, 0.2807, 0.1071],\n    [1, 13.44, 21.58, 86.18, 563, 0.08162, 0.06031, 0.0311, 0.02031, 0.1784, 0.05587, 0.2385, 0.8265, 1.572, 20.53, 0.00328, 0.01102, 0.0139, 0.006881, 0.0138, 0.001286, 15.93, 30.25, 102.5, 787.9, 0.1094, 0.2043, 0.2085, 0.1112, 0.2994, 0.07146],\n    [1, 10.95, 21.35, 71.9, 371.1, 0.1227, 0.1218, 0.1044, 0.05669, 0.1895, 0.0687, 0.2366, 1.428, 1.822, 16.97, 0.008064, 0.01764, 0.02595, 0.01037, 0.01357, 0.00304, 12.84, 35.34, 87.22, 514, 0.1909, 0.2698, 0.4023, 0.1424, 0.2964, 0.09606],\n    [1, 19.07, 24.81, 128.3, 1104, 0.09081, 0.219, 0.2107, 0.09961, 0.231, 0.06343, 0.9811, 1.666, 8.83, 104.9, 0.006548, 0.1006, 0.09723, 0.02638, 0.05333, 0.007646, 24.09, 33.17, 177.4, 1651, 0.1247, 0.7444, 0.7242, 0.2493, 0.467, 0.1038],\n    [1, 13.28, 20.28, 87.32, 545.2, 0.1041, 0.1436, 0.09847, 0.06158, 0.1974, 0.06782, 0.3704, 0.8249, 2.427, 31.33, 0.005072, 0.02147, 0.02185, 0.00956, 0.01719, 0.003317, 17.38, 28, 113.1, 907.2, 0.153, 0.3724, 0.3664, 0.1492, 0.3739, 0.1027],\n    [1, 13.17, 21.81, 85.42, 531.5, 0.09714, 0.1047, 0.08259, 0.05252, 0.1746, 0.06177, 0.1938, 0.6123, 1.334, 14.49, 0.00335, 0.01384, 0.01452, 0.006853, 0.01113, 0.00172, 16.23, 29.89, 105.5, 740.7, 0.1503, 0.3904, 0.3728, 0.1607, 0.3693, 0.09618],\n    [1, 18.65, 17.6, 123.7, 1076, 0.1099, 0.1686, 0.1974, 0.1009, 0.1907, 0.06049, 0.6289, 0.6633, 4.293, 71.56, 0.006294, 0.03994, 0.05554, 0.01695, 0.02428, 0.003535, 22.82, 21.32, 150.6, 1567, 0.1679, 0.509, 0.7345, 0.2378, 0.3799, 0.09185],\n    [-1, 8.196, 16.84, 51.71, 201.9, 0.086, 0.05943, 0.01588, 0.005917, 0.1769, 0.06503, 0.1563, 0.9567, 1.094, 8.205, 0.008968, 0.01646, 0.01588, 0.005917, 0.02574, 0.002582, 8.964, 21.96, 57.26, 242.2, 0.1297, 0.1357, 0.0688, 0.02564, 0.3105, 0.07409]\n]\n\ndiagnoses_test = [\n    [-1, 8.95, 15.76, 58.74, 245.2, 0.09462, 0.1243, 0.09263, 0.02308, 0.1305, 0.07163, 0.3132, 0.9789, 3.28, 16.94, 0.01835, 0.0676, 0.09263, 0.02308, 0.02384, 0.005601, 9.414, 17.07, 63.34, 270, 0.1179, 0.1879, 0.1544, 0.03846, 0.1652, 0.07722],\n    [1, 15.22, 30.62, 103.4, 716.9, 0.1048, 0.2087, 0.255, 0.09429, 0.2128, 0.07152, 0.2602, 1.205, 2.362, 22.65, 0.004625, 0.04844, 0.07359, 0.01608, 0.02137, 0.006142, 17.52, 42.79, 128.7, 915, 0.1417, 0.7917, 1.17, 0.2356, 0.4089, 0.1409],\n    [-1, 11.34, 21.26, 72.48, 396.5, 0.08759, 0.06575, 0.05133, 0.01899, 0.1487, 0.06529, 0.2344, 0.9861, 1.597, 16.41, 0.009113, 0.01557, 0.02443, 0.006435, 0.01568, 0.002477, 13.01, 29.15, 83.99, 518.1, 0.1699, 0.2196, 0.312, 0.08278, 0.2829, 0.08832],\n    [1, 20.92, 25.09, 143, 1347, 0.1099, 0.2236, 0.3174, 0.1474, 0.2149, 0.06879, 0.9622, 1.026, 8.758, 118.8, 0.006399, 0.0431, 0.07845, 0.02624, 0.02057, 0.006213, 24.29, 29.41, 179.1, 1819, 0.1407, 0.4186, 0.6599, 0.2542, 0.2929, 0.09873],\n    [-1, 12.36, 18.54, 79.01, 466.7, 0.08477, 0.06815, 0.02643, 0.01921, 0.1602, 0.06066, 0.1199, 0.8944, 0.8484, 9.227, 0.003457, 0.01047, 0.01167, 0.005558, 0.01251, 0.001356, 13.29, 27.49, 85.56, 544.1, 0.1184, 0.1963, 0.1937, 0.08442, 0.2983, 0.07185],\n    [1, 21.56, 22.39, 142, 1479, 0.111, 0.1159, 0.2439, 0.1389, 0.1726, 0.05623, 1.176, 1.256, 7.673, 158.7, 0.0103, 0.02891, 0.05198, 0.02454, 0.01114, 0.004239, 25.45, 26.4, 166.1, 2027, 0.141, 0.2113, 0.4107, 0.2216, 0.206, 0.07115],\n    [-1, 9.777, 16.99, 62.5, 290.2, 0.1037, 0.08404, 0.04334, 0.01778, 0.1584, 0.07065, 0.403, 1.424, 2.747, 22.87, 0.01385, 0.02932, 0.02722, 0.01023, 0.03281, 0.004638, 11.05, 21.47, 71.68, 367, 0.1467, 0.1765, 0.13, 0.05334, 0.2533, 0.08468],\n    [1, 20.13, 28.25, 131.2, 1261, 0.0978, 0.1034, 0.144, 0.09791, 0.1752, 0.05533, 0.7655, 2.463, 5.203, 99.04, 0.005769, 0.02423, 0.0395, 0.01678, 0.01898, 0.002498, 23.69, 38.25, 155, 1731, 0.1166, 0.1922, 0.3215, 0.1628, 0.2572, 0.06637],\n    [-1, 12.63, 20.76, 82.15, 480.4, 0.09933, 0.1209, 0.1065, 0.06021, 0.1735, 0.0707, 0.3424, 1.803, 2.711, 20.48, 0.01291, 0.04042, 0.05101, 0.02295, 0.02144, 0.005891, 13.33, 25.47, 89, 527.4, 0.1287, 0.225, 0.2216, 0.1105, 0.2226, 0.08486],\n    [1, 16.6, 28.08, 108.3, 858.1, 0.08455, 0.1023, 0.09251, 0.05302, 0.159, 0.05648, 0.4564, 1.075, 3.425, 48.55, 0.005903, 0.03731, 0.0473, 0.01557, 0.01318, 0.003892, 18.98, 34.12, 126.7, 1124, 0.1139, 0.3094, 0.3403, 0.1418, 0.2218, 0.0782],\n    [-1, 14.26, 19.65, 97.83, 629.9, 0.07837, 0.2233, 0.3003, 0.07798, 0.1704, 0.07769, 0.3628, 1.49, 3.399, 29.25, 0.005298, 0.07446, 0.1435, 0.02292, 0.02566, 0.01298, 15.3, 23.73, 107, 709, 0.08949, 0.4193, 0, 0.1503, 0.07247, 0.2438, 0.08541],\n    [1, 20.6, 29.33, 140.1, 1265, 0.1178, 0.277, 0.3514, 0.152, 0.2397, 0.07016, 0.726, 1.595, 5.772, 86.22, 0.006522, 0.06158, 0.07117, 0.01664, 0.02324, 0.006185, 25.74, 39.42, 184.6, 1821, 0.165, 0.8681, 0.9387, 0.265, 0.4087, 0.124]\n]\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-28T04:02:32.901545Z","iopub.execute_input":"2024-05-28T04:02:32.902616Z","iopub.status.idle":"2024-05-28T04:02:32.980424Z","shell.execute_reply.started":"2024-05-28T04:02:32.902559Z","shell.execute_reply":"2024-05-28T04:02:32.979025Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## What to Do\n \nDemonstrate your understanding and ability to have synthesized course concepts by implementing a `test` function for the perceptron. Your goal is to implement the `test` function, stubbed for you below, in the subclass `TestablePerceptron`. The `test` function should return the accuracy rate of the perceptron's prediction, given `diagnoses_test`. In the end, your work should reflect the principles seen thus far in the course.\n\nPlease be sure to demonstrate:\n\n1. Your implementation, as code, in the subclass below.\n2. Using your test function, which is already done for you, following the class definition.\n\n## Tips\n\n- Be sure that you have spent time with the Exploration materials in this course.\n- Ask questions on the course forum if you get stuck (describe what you are trying to do, and errors that you encounter)\n- Keep it simple. This is quite straightforward.\n- Be sure to run the code cell containing your TestablePerceptron class and the code cell that invokes the `test` method, or use the *>> Run All* button.","metadata":{}},{"cell_type":"markdown","source":"## Begin Your Implementation Here\n\nIn the code cell below, implement the `test` function by replacing `pass` with your code. It should accept a training set as input, and *return* a number representing the accuracy of the perceptron based on predictions made with the test set.","metadata":{}},{"cell_type":"code","source":"class TestablePerceptron(Perceptron):\n    \n  def test(self, test_set):\n        class_labels = [i[0] for i in test_set]\n        y_pred = [self.predict(i[1:]) for i in test_set]\n        correct_predictions = sum(1 for true_label, pred_label in zip(class_labels, y_pred) if true_label == pred_label)\n        accuracy = correct_predictions / len(test_set)\n        return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-28T04:02:32.983118Z","iopub.execute_input":"2024-05-28T04:02:32.983779Z","iopub.status.idle":"2024-05-28T04:02:32.999932Z","shell.execute_reply.started":"2024-05-28T04:02:32.983723Z","shell.execute_reply":"2024-05-28T04:02:32.998216Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"perceptron = TestablePerceptron(alpha = 0.1, max_epochs = 10000)\nperceptron.train(diagnoses_training)\nprint(perceptron.test(diagnoses_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T04:02:33.001631Z","iopub.execute_input":"2024-05-28T04:02:33.002035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Your TestablePerceptron should result in the following code, which trains and tests a TestablePerceptron, running without error. You do not need to modify the code below, but once your `test` method above is complete, you should see the accuracy of the test printed, instead of `None`.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion of Part 1\nThe test method first iterates over (or \"loops\" over) each sample in the test set data, before generating a prediction using these data. These steps are necessary because they provide a metric for model performance and effectiveness of each predictive output, which is the ultiamte goal and can only be trusted with a high level of accuracy. The test model then records the accuracy (**in this case, meeting convergence at 1.0 or 100%**) and reports the number of learning epochs (\"trials\") used to meet convergence (**4,719 epochs, which is subject to change due to the random input of example data and initial weights for the test method**) and testing error (**.9166 or 91.67%**), suggesting these data are linearly seperable. The learning epoch number changes each time the above code cell runs because the predictive nature of the algorithm is initally randomly weighted, and meeting convergence can occur more or less rapidly depending on the random order the data examples are presented to the test method.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Mine Detection\n\nIn this, the second, part of this notebook, you will build a classifier that can predict whether or not a sonar signature is from a mine or a rock. We'll use a version of the [sonar data set](https://www.openml.org/search?type=data&sort=runs&id=40&status=active) by Gorman and Sejnowski. Take a moment now to [familiarize yourself with the subject matter of this data set](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9e7f15d370f1edebe4627da369d953f599f45bb5), and look at the details of the version of this data set, [Mines vs Rocks, hosted on Kaggle](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks).\n\nUnlike previous notebooks, where we provide code for each step of the ML process, this notebook expects each student to implement the ML workflow steps. We will get you started by providing the first step, loading the data, and providing some landmarks below. Your process should demonstrate:\n\n1. Loading the data\n2. Exploring the data\n3. Preprocessing the data\n4. Preparing the training and test sets\n5. Creating and configuring a sklearn.linear_model.Perceptron\n6. Training the perceptron\n7. Testing the perceptron\n8. Demonstrating making predictions\n9. Evaluate (and Improve) the results\n\nCan you train a classifier that can predict whether a sonar signature is from a mine or a rock? \"Three trained human subjects were each tested on 100 signals, chosen at random from the set of 208 returns used to create this data set. Their responses ranged between 88% and 97% correct.\" Can your classifier outperform the human subjects?\n\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Load the Data\n\nThe notebook comes pre-bundled with the [Mines vs Rocks data set](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks). Our first step is to create a pandas DataFrame from the CSV file. Note that the CSV file has no header row. Loading the CSV file into a DataFrame will make it easy for us to explore the data, preprocess it, and split it into training and test sets.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nsonar_csv_path = \"../input/mines-vs-rocks/sonar.all-data.csv\"\nsonar_data = pd.read_csv(sonar_csv_path, header=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a pandas DataFrame encapsulating the sonar data, and can proceed with our data exploration.","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Explore the Data\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Seeking a Notebook \"pretty-print\" for these data, to assess patterns in these sonar data at a glance, I use the head function to visually inspect variables. I am investigating whether or not there are any obvious trends simply by looking at these data. In this case, I do not see any acutely obvious patterns or data problems. I need to take a closer look.","metadata":{}},{"cell_type":"code","source":"# View Data\nsonar_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I examine the shape of these data, because it provides insight into the data characteristics. For example, if there are notable imbalances or if computational size will be challenging during train/testing methods. I have learned there are 208 samples and 61 columns (one of which is not a sonar device and the column is instead a classifier for rock or cylinder).","metadata":{}},{"cell_type":"code","source":"# Check the shape of the dataset\nsonar_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I check these data for any NAs (missing values), which I will remove from these data if found. I did not find NAs impeding these data. The data type is floating, and needs to be an integer for later processing.","metadata":{}},{"cell_type":"code","source":"sonar_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#NAs\nsonar_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am curious about the distribution of these data across the classes Metal Cylinder/Mine (M) and Rock (R), and find there appear to be more Mines detected than Rocks, but only by 14 samples (111 Mines and 97 Rocks). Meaning, the distribution is relatively representative for each class.","metadata":{}},{"cell_type":"code","source":"sonar_data[60].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I explored individual data distributions/ranges for each feature to determine if these data need to be standardized/normalized. Here, I determine standardization is necessary as the features exhibit variation in their distrubitions and are not yet centralized around 0. The need for standardization appears to be the only critical pre-processing step for these data.","metadata":{}},{"cell_type":"code","source":"# Exploring normalization and standardization\nplt.figure(figsize=(15, 10))\nfor i in range(60):\n    plt.subplot(6, 10, i + 1)\n    plt.hist(sonar_data[i], bins=20, color='purple', alpha=0.7)\n    plt.title(f'Feature {i}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### After exploring this sonar data set, we know that:\n\n- There are 208 samples, each representing either a rock or a metal cylinder detected with underwater sonar equipment.\n- There are 60 conceptual features in the data set, the sonar units which were clamped to the amplitude value of the signal to be classified.\n- The features are all numeric, and each have their own distinct measures, ranges and distributions (ranging from 0.0 to 1.0).\n- There are no missing values/ NAs.\n- These data have normal distribution.\n- Each number represents the energy within a particular frequency band, integrated over a set time frame.\n- The sonar detection represents the class label for each underwater item determination, indicated by an 'M' for Metal Cylinder/Mine or a 'R' for Rock.","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Preprocess the Data\n\n","metadata":{}},{"cell_type":"markdown","source":"I am replacing the class values to 1 and -1 for the Metal Cylinder/Mine versus Rock, making these simple numeric binary classifiers for later processing in the training/testing dataset.","metadata":{}},{"cell_type":"code","source":"sonar_data[60].replace({'M': 1, 'R': -1}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to ensure all of the data are integers, prior to creating our perceptron.","metadata":{}},{"cell_type":"code","source":"#convert string column to float, sourced from https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\ndef str_column_to_float(sonar_data, column):\n    for row in sonar_data:\n        row[column] = float(row[column].strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert string column to integer, sourced from https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\ndef str_column_to_int(sonar_data, column):\n    class_values = [row[column] for row in sonar_data]\n    unique = set(class_values)\n    lookup = dict()\n    for i, value in enumerate(unique):\n        lookup[value] = i\n        for row in sonar_data:\n            row[column] = lookup[row[column]]\n            return lookup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I am quickly checking to make sure no new NA/missing values are introduced into these data. It looks great.","metadata":{}},{"cell_type":"code","source":"NA_values = sonar_data.isna().sum()\nNA_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although I did not find obvious need for advanced pre-processing (i.e. NAs are not present in these data, the classes are relatively balanced in sample size for the training data), often algorithms using gradient descent benefit from standardization. Therefore, I am applying standardization prior to instantiating the classifiers, and after exploring individual feature classes exhibiting variation in feature scaling, standardization is determined as an important step for preprocessing.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n#apply transformation for standardization\nsonar_data_scaled = scaler.fit_transform(sonar_data.iloc[:, :-1])\nsonar_data_standard = pd.DataFrame(sonar_data_scaled, columns=sonar_data.columns[:-1])\n\n#view\nsonar_data_standard.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After standardizing these data, the range of values for each feature center around 0 and restrict the data spread, narrowing the range across the features. When we re-visit our distribution plots for each feature, we can visually explore the outcome of this pre-processing step.","metadata":{}},{"cell_type":"code","source":"# Exploring standardization\nplt.figure(figsize=(15, 10))\nfor i in range(60):\n    plt.subplot(6, 10, i + 1)\n    plt.hist(sonar_data_standard[i], bins=20, color='purple', alpha=0.7)\n    plt.title(f'Feature {i}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Prepare the Training and Test Data Sets\n\nUsing the`train_test_split` approach, I passed these standardizaed sonar data with to 75% training set, and 25% to the testing set. I excluded the final column of data because the 0,1 binary in that column is assigning either the Mine or Rock classification, and should not be included in the training/test data sets. The random state was arbitarily selected at \"50\" based on online examples, but is essentially determining where the split will occur each time in these data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#excludes the Mine/Rock binary classification column\nX = sonar_data_standard.iloc[:, :-1]\ny = sonar_data.iloc[:, -1]\n\n#75/25 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50)#random state selected from example, from my understanding it can be set to any value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Instantiate and Configure a Perceptron\n\nThis initial attempt at the linear perceptron uses select default settings described in the sklearn.linear_model resource. That being said, I am stuck on this step because the p.fit step used to fit these data to the new training data is unable to navigate a label type. This is perplexing for me because I went through the code and assigned all data types to numeric in the pre-processing phase. I am aware there are floating data types from the data exploration, and was successfully (I think?) executing code to categorize those data types as numeric. Without being able to fit these data to the train X and y, I am unable to proceed.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\np = Perceptron(tol=0.001, max_iter=10000, alpha=0.0001, early_stopping=True)\np.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Train the Perceptron\n\nThe stochastic gradient descent requires two parameters, the learning rate (the limit for each weighted update) and epochs (number of times iterated over training data with weight updates). If I were able to proceed, I would use code similar to this (found in provided course resources) to weight the training dataset. However, these data are balanced (both classes are well represented in these data), and therefore ","metadata":{}},{"cell_type":"code","source":"def train(self, training_set):\n    self.weights = self._initial_weights(len(training_set[0]))\n    for i in range(self.max_epochs):\n        for record in training_set:\n            y = record[0]\n            y_hat = self.predict(record[1:])\n            f\"Epoch {i + 1}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Test the Perceptron\n\nUnable to attempt.\n","metadata":{}},{"cell_type":"code","source":"def predict(row, weights):\n    activation = weights[0]\n    for i in range(len(row)-1):\n        activation += weights[i + 1] * row[i]\n        return 1.0 if activation >= 0.0 else 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"STEP 8","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nnew_record = np.array([0.02, 0.45, 0.78, 0.12, 0.65, 0.88, 0.44, 0.33, 0.77, 0.99,\n                            ##......(fill in the rest)##\n                            0.55, 0.33, 0.11, 0.15, 0.46, 0.93, 0.13, 0.45, 0.89, 1])\nnew_record = new_record.reshape(1,60)\nprediction = classifier.predict(new_record)\nprint(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 9: Evaluate (and Improve?)\n\nDescribe the configuration and performance of your classifier, and what the results mean. Is this a good classifier? Why isn't it better than what it is? What might you try next to improve it? How accurate can you make your classifier? If you have time, see if you can increase the accuracy. Can you beat 98%? 🙀\n\nA good classifier is one that approached convergence with a low error rate and high accuracy (as close to 100% as possible). Therefore, if I were able to make it to this step, I would provide experimental adjustments to the linear Perceptron model and explore outputs until enhancing the accuracy to nearly 100%. If convergence is impossible, it is possible these data are not linearly separatable, and therefore a new dimension may need to be applied to improve the outputs (similar to the lecture video that applied a new scale of Pie and the data falling on the axis for proximity to a given radius). That being said, I am directionless without first successfully training and testing the Perceptron.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\np = Perceptron(tol=0.001, eta0=0.001, max_iter=10000, alpha=0.0001, early_stopping=True) #this is an example based on defaults that can be adjusted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nGive us a quick recap of what you've done here in **Part 2**. Mention _three_ things that were most notable in this process, whether it's related to exploration, preprocessing, configuring, training, or evaluating. If you put in the effort to try to improve the perceptron, describe what you did and what led you to try what you did, and describe the results. Conclude with some questions about the problem or what you might do next to increase the performance of your classifier.\n\nI was unfortunately hung-up on **Step 5: Instantiate and Configure a Perceptron**. I did not have trouble with the linear model of the perceptron, and if I was able to produce results, adjusting the values in this model would have been my \"go-to\" for finding a higher accuracy rate. That being said, from the steps I was able to accomplish, I think it is note worthy that a data type appears to be trippping up my coding, despite assigning all data types to numeric outputs. \n\n\n","metadata":{}}]}