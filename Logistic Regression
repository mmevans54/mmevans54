{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 1: Implementing Logistic Regression\n\nGiven a nearly-complete LogisticRegressor, and a simple training set of tumor data, demonstrate your ability to implement a logistic regression model's `fit` function, such that it properly trains its model using gradient descent.\n\n## The LogisticRegressor\n\nLet's first review the LogisticRegressor, which you should find familiar. Notice that the `fit` method uses a fixed number of iterations, only for simplicity and experimentation, and is stubbed to do nothing. But, also notice that the comments in `fit` describe a training process using gradient descent.\n\nRun the code cell and observe the results.","metadata":{}},{"cell_type":"code","source":"import math\n\nclass LogisticRegressor:\n\n    def __init__(self, w = 0, b = 0, alpha = 0.1):\n        self.w = w\n        self.b = b\n        self.alpha = alpha\n\n    def fit(self, x_train, y_train):\n        delta_w = [0,0]\n        for _ in range(0, 100000):\n            # Determine the changes that need to be made to w\n            delta_w = self._d_cost_function_w(x_train, y_train)\n            # Determine the changes that need to be made to b\n            delta_b = self._d_cost_function_b(x_train, y_train)\n            # For each weight element in the w vector\n            for weight in range(len(delta_w)):\n                delta_w[weight] = delta_w[weight] * self.alpha\n                # Update the weight (subtract the change times alpha)\n                self.w[weight] = self.w[weight] - delta_w[weight]\n            # Update the bias (subtract the change times alpha)\n            self.b = self.b - delta_b*self.alpha\n            \n\n    def cost(self, x_examples, y_class_labels):\n        cost = 0\n        for i in range(len(x_examples)):\n            cost += self._loss(x_examples[i], y_class_labels[i])\n        return cost / len(x_examples)\n\n    def _loss(self, x, y):\n        z = self._dot_product(self.w, x) + self.b\n        return -y * math.log(self._sigmoid(z)) - (1 - y) * math.log(1- self._sigmoid(z))\n\n    def _d_cost_function_w(self, x_train, y_train):\n        delta_w = [0] * len(x_train[0])\n        for i in range(len(x_train)):\n            error = self._sigmoid(self._dot_product(self.w, x_train[i]) + self.b) - y_train[i]\n            for j in range(len(delta_w)):\n                delta_w[j] += error * x_train[i][j]\n        for i in range(len(delta_w)):\n            delta_w[i] = delta_w[i] / len(x_train)\n        return delta_w\n\n    def _d_cost_function_b(self, x_train, y_train):\n        delta_b = 0\n        for i in range(len(x_train)):\n            delta_b += self._sigmoid(self._dot_product(self.w, x_train[i]) + self.b) - y_train[i]\n        return delta_b / len(x_train)\n\n    def predict(self, x):\n        if self._sigmoid( self._dot_product(self.w, x) + self.b) >= 0.5:\n            return 1\n        else:\n            return 0\n\n    def _dot_product(self, a, b):\n        return sum(pair[0] * pair[1] for pair in zip(a, b))\n\n    def _sigmoid(self, exponent):\n        return 1 / (1 + math.exp(-exponent))\n\n\n\nx_train = [[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]\ny_train = [0, 0, 0, 1, 1, 1]\n\nregressor = LogisticRegressor([0, 0], 0, 0.1)\nregressor.fit(x_train, y_train)\n\nfor example in x_train:\n    print(f\"Prediction for {example} is {regressor.predict(example)}\")\n\nprint(regressor.cost(x_train, y_train))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:03.253648Z","iopub.execute_input":"2024-05-16T12:09:03.254067Z","iopub.status.idle":"2024-05-16T12:09:07.525207Z","shell.execute_reply.started":"2024-05-16T12:09:03.254029Z","shell.execute_reply":"2024-05-16T12:09:07.524052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the output, our classifier is currently predicting only a 1, and its cost is about 0.69.\n\n## What to Do\n\nYour goal is to implement, in the code cell above, the `fit` function. When complete, you should see results identical to the output shown at the end of the *Exploration: Applying Logistic Regression*.\n\n1. Implement the `fit` function in the code cell above.\n2. Run the code cell frequently, and observe the output.\n3. When you believe your implementation is complete, increase the number of iterations. Compare your output to what we have seen in the corresponding Exploration.\n4. Rely on the functions that are already implemented for you, such as `_d_cost_function_b` and `_d_cost_function_w`.\n\nThe best tip for thinking about this challenge is to become intimately familiar with the process of gradient descent, and recognizing what `_d_cost_function_b` and `_d_cost_function_w` return. **Use the comments in the `fit` function as a general guide, not a literal line-by-line translation into code.**\n\nYou'll know your implementation is sound when the output of the code cell matches what we have seen in the *Exploration: Applying Logistic Regression*.","metadata":{}},{"cell_type":"markdown","source":"## ðŸ’¡ Conclusion\n\nTo implement the \"fit\" method for the LogisticRegressor I first looked through the other methods from the Exploration page to familiarize myself with what steps they were performing. I then used the guideline comments in the method to implement each step in the \"fit\" method. I added print statements along the way to make sure that the delta_w and delta_b variables were being updated as expected, but removed them once I had confirmed that. From last week, I remembered that I needed to make sure that each delta_w was updated, and then to apply those updates to each w in a loop so that they were done individually. I then stepped up the range in the for loop with the same increments that were in the Exploration and compared the outputs to make sure that I was getting the correct values. My end result values are the same as the Exploration page with a cost of 0.0016 and an accuracy of 100% as all values were predicted correctly. It looks like I have succesfully implemented the \"fit\" method for the LogisticRegressor.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Classifying Fashion Items\n\nIn this, the second, part of this notebook, you will observe a non-annotated implementation of a machine learning process, and enhance it with descriptive markdown cells and additional code. Your goal is to narrate and improve an experiment that measures the performance of the [scikit-learn LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model for classifying images of fashion items. We'll use the popular [Fashion MNIST data set](https://github.com/zalandoresearch/fashion-mnist) by Xiao, Rasul, and Vollgraf. Take a moment now to [familiarize yourself with the version of this data set](), and also take a look at [a version of this data on Kaggle](https://www.kaggle.com/datasets/zalando-research/fashionmnist).\n\nUnlike prior notebooks, in which you are given either a guided framework of steps, or provided explicit code to try, in this notebook the code shall be your framework. Your goal is to break apart this one big code cell into a cohesive, multi-section, narrated Notebook, that guides the reader through the machine learning process. You have seen and practiced this in prior Notebooks, and you are encouraged to replicate the spirit of our past work here.\n\nIn other words, below you have a bunch of code. Your goal is to:\n\n1. Narrate a machine learning process\n2. Explain what the code is doing, and add to it as necessary\n3. Experiment, tune the model, and discuss your results\n\nYour Notebook should consist of many sections, with each section representing a step in the machine learning process. The first section, **Problem Statement**, has been completed for you. Each section should start with a markdown cell containing a descriptive second-level header, and at least a few sentences that prepare the reader for what the purpose of the step is.\n\nEach section should consist of both prose, in markdown cells, and code cells. Almost every section should consist of multiple markdown and code cells. You should often add to the provided code. For example, if you have a section on exploring data, you should probably do more than just look at the `head` and `shape`.\n\nYour first step is to run the code block, and spend time with each line of code to discern how it reflects some unit of work in our machine learning process.\n\nIn the end, demonstrate how you modify the experiment and/or tune the model to increase the accuracy of the model. (Spend time with the [official documentation of the data set](https://github.com/zalandoresearch/fashion-mnist). What is the human accuracy score? Can your model surpass it when validated with the complete training set?)\n","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nOur goal is to automate the identification of images of ten different kinds of fashion items, from t-shirts to ankle boots. To do so, we will attempt to train a logistic regression model using a well-prepared data set of images of fashion items. Our goal is to tune our end-to-end machine learning process, and to tune our classification model, to see how accurately it may predict the correct class label of different fashion items.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Import Libraries\n\nTo start work on our classification problem, we will first import the libraries and functions that we will use in our in creating our model. We will use the pandas library functions for data manipulation and the sklearn functions for splitting our data into our test and training sets, and creating and testing our model. Lastly, we will use matplotlib for any plotting that we want to do to visualize our data or our model. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:07.526801Z","iopub.execute_input":"2024-05-16T12:09:07.52715Z","iopub.status.idle":"2024-05-16T12:09:07.53429Z","shell.execute_reply.started":"2024-05-16T12:09:07.527122Z","shell.execute_reply":"2024-05-16T12:09:07.532944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Load the data","metadata":{}},{"cell_type":"markdown","source":"Before we import the data, we will create a couple of items for later use: a dictionary of the labels we will use for the fashion items, and a function which will plot the images. ","metadata":{}},{"cell_type":"code","source":"labels = {\n    0: 'T-shirt / Top',\n    1: 'Trouser',\n    2: 'Pullover',\n    3: 'Dress',\n    4: 'Coat',\n    5: 'Sandal',\n    6: 'Shirt',\n    7: 'Sneaker',\n    8: 'Bag',\n    9: 'Ankle Boot'\n}\n\ndef display_image(features, numeric_label):\n    print(f\"Class: {labels[numeric_label]}\")\n    plt.imshow(features.reshape(28, 28))","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:07.53575Z","iopub.execute_input":"2024-05-16T12:09:07.536117Z","iopub.status.idle":"2024-05-16T12:09:07.547345Z","shell.execute_reply.started":"2024-05-16T12:09:07.536078Z","shell.execute_reply":"2024-05-16T12:09:07.546142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will read in the csv files containing the training and test data sets.","metadata":{}},{"cell_type":"code","source":"fashion_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\nfashion_test_set = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:07.550498Z","iopub.execute_input":"2024-05-16T12:09:07.55088Z","iopub.status.idle":"2024-05-16T12:09:15.8384Z","shell.execute_reply.started":"2024-05-16T12:09:07.550848Z","shell.execute_reply":"2024-05-16T12:09:15.837072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Explore the data","metadata":{}},{"cell_type":"markdown","source":"Now that we have loaded our data sets, we will look at the shape of each data set and observe the first few rows of data. ","metadata":{}},{"cell_type":"code","source":"fashion_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:15.840195Z","iopub.execute_input":"2024-05-16T12:09:15.840548Z","iopub.status.idle":"2024-05-16T12:09:15.849976Z","shell.execute_reply.started":"2024-05-16T12:09:15.84052Z","shell.execute_reply":"2024-05-16T12:09:15.848197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashion_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:15.851759Z","iopub.execute_input":"2024-05-16T12:09:15.852243Z","iopub.status.idle":"2024-05-16T12:09:15.876473Z","shell.execute_reply.started":"2024-05-16T12:09:15.852201Z","shell.execute_reply":"2024-05-16T12:09:15.875142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data set consists of 60,000 data objects with 785 features, or columns. The first column is the class label for that data object, each number corresponds to a fashion item from our \"labels\" dictionary above. The remaining 784 columns each correspond to a pixel in the image, each row makes up all of the pixels in an image. The data set page on Kaggle confirms that each images is a 28 x 28 pixel grayscale image.","metadata":{}},{"cell_type":"code","source":"fashion_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:15.879165Z","iopub.execute_input":"2024-05-16T12:09:15.879628Z","iopub.status.idle":"2024-05-16T12:09:19.717411Z","shell.execute_reply.started":"2024-05-16T12:09:15.879582Z","shell.execute_reply":"2024-05-16T12:09:19.716075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashion_data.max().max()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.718542Z","iopub.execute_input":"2024-05-16T12:09:19.719067Z","iopub.status.idle":"2024-05-16T12:09:19.764208Z","shell.execute_reply.started":"2024-05-16T12:09:19.719037Z","shell.execute_reply":"2024-05-16T12:09:19.762855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashion_data.min().min()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.765607Z","iopub.execute_input":"2024-05-16T12:09:19.765943Z","iopub.status.idle":"2024-05-16T12:09:19.811561Z","shell.execute_reply.started":"2024-05-16T12:09:19.765899Z","shell.execute_reply":"2024-05-16T12:09:19.810217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashion_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.817269Z","iopub.execute_input":"2024-05-16T12:09:19.817688Z","iopub.status.idle":"2024-05-16T12:09:19.867924Z","shell.execute_reply.started":"2024-05-16T12:09:19.817652Z","shell.execute_reply":"2024-05-16T12:09:19.866674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum value in the data set is 255 and the minimum value is 0. The data set page on Kaggle indicates that these values are representations of the darkness of a pixel, with 0 being the lightest pixel and 255 being the darkest pixel. Looking at the data types confirms that all data types are ints. ","metadata":{}},{"cell_type":"code","source":"label_counts = fashion_data[\"label\"].value_counts()\nlabel_counts","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.869608Z","iopub.execute_input":"2024-05-16T12:09:19.870511Z","iopub.status.idle":"2024-05-16T12:09:19.881038Z","shell.execute_reply.started":"2024-05-16T12:09:19.870467Z","shell.execute_reply":"2024-05-16T12:09:19.879804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like all classes are represented equally in the training data set. Now we will take a look at the test data set.","metadata":{}},{"cell_type":"code","source":"fashion_test_set.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.882597Z","iopub.execute_input":"2024-05-16T12:09:19.882981Z","iopub.status.idle":"2024-05-16T12:09:19.891103Z","shell.execute_reply.started":"2024-05-16T12:09:19.882946Z","shell.execute_reply":"2024-05-16T12:09:19.890113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashion_test_set.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.892454Z","iopub.execute_input":"2024-05-16T12:09:19.892799Z","iopub.status.idle":"2024-05-16T12:09:19.919091Z","shell.execute_reply.started":"2024-05-16T12:09:19.89277Z","shell.execute_reply":"2024-05-16T12:09:19.917792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test data set consists of 10,000 data objects, each with 1 labels and 784 features, each of which is a pixel in an image. We will keep the test data set for later to validate our model.","metadata":{}},{"cell_type":"markdown","source":"One of our first steps was to write a function to display the images, we will now use this function to take a look at what a few of the images look like. However, our function takes data that has the class label in a separate data frame, so we will split off the class label column into a separate data frame now.","metadata":{}},{"cell_type":"code","source":"X = fashion_data[fashion_data.columns[1:]]\nY = fashion_data['label']\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:19.921064Z","iopub.execute_input":"2024-05-16T12:09:19.921444Z","iopub.status.idle":"2024-05-16T12:09:20.105194Z","shell.execute_reply.started":"2024-05-16T12:09:19.921412Z","shell.execute_reply":"2024-05-16T12:09:20.104127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:20.106407Z","iopub.execute_input":"2024-05-16T12:09:20.106737Z","iopub.status.idle":"2024-05-16T12:09:20.116074Z","shell.execute_reply.started":"2024-05-16T12:09:20.106708Z","shell.execute_reply":"2024-05-16T12:09:20.11479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the first few rows of both X and Y confirms that we split the data frames as expected. We will now use the display_image function that we created above to visualize a few different data objects. Each column in the data object row corresponds to a pixel in the image, when we plot the image with our function, we can see what looks to be like a pullover in the first example, confirmed by the label above the image. ","metadata":{}},{"cell_type":"code","source":"display_image(X.loc[0].values, Y.loc[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:20.117642Z","iopub.execute_input":"2024-05-16T12:09:20.118097Z","iopub.status.idle":"2024-05-16T12:09:20.338459Z","shell.execute_reply.started":"2024-05-16T12:09:20.118066Z","shell.execute_reply":"2024-05-16T12:09:20.337299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the second example we see what a T-shirt/Top looks like.","metadata":{}},{"cell_type":"code","source":"display_image(X.loc[10].values, Y.loc[10])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:20.341394Z","iopub.execute_input":"2024-05-16T12:09:20.342056Z","iopub.status.idle":"2024-05-16T12:09:20.539934Z","shell.execute_reply.started":"2024-05-16T12:09:20.341985Z","shell.execute_reply":"2024-05-16T12:09:20.538594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, in the third example we see what a Sneaker might look like. ","metadata":{}},{"cell_type":"code","source":"display_image(X.loc[59999].values, Y.loc[59999])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:20.541492Z","iopub.execute_input":"2024-05-16T12:09:20.54196Z","iopub.status.idle":"2024-05-16T12:09:20.746206Z","shell.execute_reply.started":"2024-05-16T12:09:20.541901Z","shell.execute_reply":"2024-05-16T12:09:20.744727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have thoroughly explored the data, we will move on to the preprocessing step.","metadata":{}},{"cell_type":"markdown","source":"## Step 4. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"In preprocessing we will prepare the data for training. Below we will divide all of the pixel columns in X by 255, the maximum value in the data set, to normalize the values.","metadata":{}},{"cell_type":"code","source":"X = X / 255\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:20.747975Z","iopub.execute_input":"2024-05-16T12:09:20.748387Z","iopub.status.idle":"2024-05-16T12:09:21.264514Z","shell.execute_reply.started":"2024-05-16T12:09:20.748354Z","shell.execute_reply":"2024-05-16T12:09:21.263733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also take a look at the image again to make sure that we can still discern the image after normalizing the data.","metadata":{}},{"cell_type":"code","source":"display_image(X.loc[1].values, Y.loc[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:21.265657Z","iopub.execute_input":"2024-05-16T12:09:21.26625Z","iopub.status.idle":"2024-05-16T12:09:21.504803Z","shell.execute_reply.started":"2024-05-16T12:09:21.266218Z","shell.execute_reply":"2024-05-16T12:09:21.503719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This image looks to be an ankle boot, as indicated by the label, confirming that the preprocessing step was done correctly. ","metadata":{}},{"cell_type":"markdown","source":"## Step 5. Prepare the test and training sets","metadata":{}},{"cell_type":"markdown","source":"We will now use the sklearn train_test_split function to divide X and Y data frames from the original training data set into a test set and a training set. This data set originally consisted of 60,000 data objects, we will divide it into a 48,000 row training data set (80% of the data) and a 12,000 row test data set (20% of the data). ","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\nx_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:21.506768Z","iopub.execute_input":"2024-05-16T12:09:21.507265Z","iopub.status.idle":"2024-05-16T12:09:22.190661Z","shell.execute_reply.started":"2024-05-16T12:09:21.507222Z","shell.execute_reply":"2024-05-16T12:09:22.189456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:22.192498Z","iopub.execute_input":"2024-05-16T12:09:22.193875Z","iopub.status.idle":"2024-05-16T12:09:22.202279Z","shell.execute_reply.started":"2024-05-16T12:09:22.193832Z","shell.execute_reply":"2024-05-16T12:09:22.20094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:22.203971Z","iopub.execute_input":"2024-05-16T12:09:22.204392Z","iopub.status.idle":"2024-05-16T12:09:22.216492Z","shell.execute_reply.started":"2024-05-16T12:09:22.204349Z","shell.execute_reply":"2024-05-16T12:09:22.214967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:22.218303Z","iopub.execute_input":"2024-05-16T12:09:22.219551Z","iopub.status.idle":"2024-05-16T12:09:22.230258Z","shell.execute_reply.started":"2024-05-16T12:09:22.219502Z","shell.execute_reply":"2024-05-16T12:09:22.228895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a training data set divided into its X and Y portions and a test data set divided into its X and Y portions and are ready to move on to training. ","metadata":{}},{"cell_type":"markdown","source":"## Step 6. Training","metadata":{}},{"cell_type":"markdown","source":"We are now ready to create a LogisticRegression model and train it with our training data. We will start with 1, 10, and 50 iterations of training and take a look at the output for each. ","metadata":{}},{"cell_type":"code","source":"for iterations in [1, 10, 50]:\n    logistic_model = LogisticRegression(max_iter = iterations)\n    logistic_model.fit(x_train, y_train)\n    y_pred = logistic_model.predict(x_test)\n\n    accuracy = accuracy_score(y_test, y_pred, normalize = True)\n    accuracy_count = accuracy_score(y_test, y_pred, normalize = False)\n    precision = precision_score(y_test, y_pred, average = 'weighted')\n    recall = recall_score(y_test, y_pred, average = 'weighted')\n\n    print(\"Number of test records:\\t\", len(y_test))\n    print(\"Accuracy count:\\t\\t\", accuracy_count)\n    print(\"Acccuracy:\\t\\t\", accuracy)\n    print(\"Precision:\\t\\t\", precision)\n    print(\"Recall:\\t\\t\\t\", recall)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:09:22.232211Z","iopub.execute_input":"2024-05-16T12:09:22.233102Z","iopub.status.idle":"2024-05-16T12:09:38.141803Z","shell.execute_reply.started":"2024-05-16T12:09:22.233059Z","shell.execute_reply":"2024-05-16T12:09:38.14012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our logistic regressor failed to converge in each of these iterations, but the accuracy is increasing in each interation so far. The precision (the ability of the classifer to not label an item as the wrong item) and the recall (the ability of the classifer to find all the shoes, for example) are also increasing. However, we still have nearly a couple of thousand items that are not classified correctly, so in the next step I will attempt to tune the classifier for better results. ","metadata":{}},{"cell_type":"markdown","source":"## Step 7. Adjusting the model","metadata":{}},{"cell_type":"markdown","source":"In this step I will attempt to improve the model by adusting the hyperparameters. I first tried to increase the max_iter to 1000 to see if that would make a large difference, it only improved the score by less than 0.1 and it took a long time to run, so I reduced it. I also tried a few different penalty, solver, and intercept options, but was not able to significantly increase the accuracy.","metadata":{}},{"cell_type":"code","source":"logistic_model = LogisticRegression(max_iter = 150)\nlogistic_model.fit(x_train, y_train)\ny_pred = logistic_model.predict(x_test)\n\naccuracy = accuracy_score(y_test, y_pred, normalize = True)\naccuracy_count = accuracy_score(y_test, y_pred, normalize = False)\nprecision = precision_score(y_test, y_pred, average = 'weighted')\nrecall = recall_score(y_test, y_pred, average = 'weighted')\n\nprint(\"Number of test records:\\t\", len(y_test))\nprint(\"Accuracy count:\\t\\t\", accuracy_count)\nprint(\"Acccuracy:\\t\\t\", accuracy)\nprint(\"Precision:\\t\\t\", precision)\nprint(\"Recall:\\t\\t\\t\", recall)\nprint(\"Iterations:\\t\\t\", logistic_model.n_iter_)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:21:22.88148Z","iopub.execute_input":"2024-05-16T12:21:22.881927Z","iopub.status.idle":"2024-05-16T12:21:58.040108Z","shell.execute_reply.started":"2024-05-16T12:21:22.881875Z","shell.execute_reply":"2024-05-16T12:21:58.038545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I was able to increase the accuracy by around 1 percentage point by increasing the max iterations to 150. I tried increasing them more than this, but the accuracy started to decrease. I was not able to increase it by changing any of the other parameters. ","metadata":{}},{"cell_type":"markdown","source":"## Step 8. Model Validation","metadata":{}},{"cell_type":"markdown","source":"Now that our model has been trained, we can try it on the test data set that we set aside in the beginning. First, we will split this test data set into the class label column, and the features columns. ","metadata":{}},{"cell_type":"code","source":"X_test_set = fashion_test_set[fashion_data.columns[1:]]\nY_test_set = fashion_test_set['label']\nX_test_set.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:22:39.218788Z","iopub.execute_input":"2024-05-16T12:22:39.21925Z","iopub.status.idle":"2024-05-16T12:22:39.274589Z","shell.execute_reply.started":"2024-05-16T12:22:39.219217Z","shell.execute_reply":"2024-05-16T12:22:39.273022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will perform prediction using this set and see what kind of results we get.","metadata":{}},{"cell_type":"code","source":"y_test_pred = logistic_model.predict(X_test_set)\n\naccuracy = accuracy_score(Y_test_set, y_test_pred, normalize = True)\naccuracy_count = accuracy_score(Y_test_set, y_test_pred, normalize = False)\nprecision = precision_score(Y_test_set, y_test_pred, average = 'weighted')\nrecall = recall_score(Y_test_set, y_test_pred, average = 'weighted')\n\nprint(\"Number of test records:\\t\", len(y_test_pred))\nprint(\"Accuracy count:\\t\\t\", accuracy_count)\nprint(\"Acccuracy:\\t\\t\", accuracy)\nprint(\"Precision:\\t\\t\", precision)\nprint(\"Recall:\\t\\t\\t\", recall)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:22:44.759559Z","iopub.execute_input":"2024-05-16T12:22:44.760001Z","iopub.status.idle":"2024-05-16T12:22:44.896223Z","shell.execute_reply.started":"2024-05-16T12:22:44.759959Z","shell.execute_reply":"2024-05-16T12:22:44.89459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing predictions with the test set gives us an accuracy of about 72%, which isn't quite as good as the training set, but our model was able to perform predictions on raw data.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook I have stepped through the machine learning process. I began with loading the data and doing some exploration to familiarize myself with it. I then normalized the data and performed an initial fit with only 1 iteration of training. The results were not very good, but after increasing the maximum training interations to 50 I was able to get decent results of approximately 85% accuracy. After this, I attempted to tune my model by setting various hyperparameters, but was not able to increase the accuracy by more than about 1%. A few things that I either learned or were confirmed for me during this process were:\n\n1. Tuning hyperparameters as this point is a shot in the dark for me because I don't understand them well enough to make educated decisions. \n2. There isn't an obvious tuning parameter for logistic regression like there was for linear regression. \n3. Sometimes I can get slightly better results, but at the cost of a lot of computing power. \n\nOur classifier was able to correctly predict about 72% of fashion items in the test data set. I would like to learn more about how to tune this model to see if I can improve the performance. Alternatively, my goal is to understand machine learning enough to know if this is actually a good result. \n\n","metadata":{}}]}